{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a2e61f5",
   "metadata": {},
   "source": [
    "## Simple Gen AI App Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0865303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "# Langsmith tracing\n",
    "os.environ[\"LANGCHAIN_PROJECT_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "105a1d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Data Ingestion -- From the website we need ot scrap the data\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "url = \"https://docs.smith.langchain.com/administration/tutorials/manage_spend\"\n",
    "loader = WebBaseLoader(web_path=(url))\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6789978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splitted_text = splitter.split_documents(docs)\n",
    "splitted_text\n",
    "len(splitted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1faebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "embedding = OpenAIEmbeddings()\n",
    "vector_db = FAISS.from_documents(splitted_text, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e5b95c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x11cf2ed90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3039fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Optimization 2: limit usage\\u200b\\nIn the previous section, we managed data retention settings to optimize existing spend. In this section, we will\\nuse usage limits to prevent future overspend.\\nLangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics we\\'ve\\nbeen tracking on our usage graph. We can use these in tandem to have granular control over spend.\\nTo set limits, we navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the\\nbottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along\\nwith a cost estimate:\\n\\nLets start by setting limits on our production usage, since that is where the majority of spend comes from.\\nSetting a good total traces limit\\u200b\\nPicking the right \"total traces\" limit depends on the expected load of traces that you will send to LangSmith. You should\\nclearly think about your assumptions before setting a limit.\\nFor example:'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"LangSmith has two usage limits: total traces and extended\"\n",
    "result = vector_db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1a089bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x11dd3c290>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11dd3de10>, root_client=<openai.OpenAI object at 0x11dd3ced0>, root_async_client=<openai.AsyncOpenAI object at 0x11dd3dbd0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a8ca45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template=\"\\nAnswer the question using only the context. If it's not in the context, say you don't know.\\n\\nQuestion:\\n{input}\\n\\nContext:\\n{context}\\n\"), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x11dd3c290>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11dd3de10>, root_client=<openai.OpenAI object at 0x11dd3ced0>, root_async_client=<openai.AsyncOpenAI object at 0x11dd3dbd0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval chain, Documents chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question using only the context. If it's not in the context, say you don't know.\n",
    "\n",
    "Question:\n",
    "{input}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "document_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "294a8c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\": \"LangSmith has two usage limits: total traces and extended\",\n",
    "    \"context\": [Document(page_content=\"LangSmith has two usage limits: total traces and extended. These correspond to the two metrics we've been tracking on our usage graph. We can use these in tandem to have granular control over spend.\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130afc2",
   "metadata": {},
   "source": [
    "### However, we want the documents to first come from the retriever we just set up. That way, we can use the retriever to dynamically select the most relevant documents and pass those in a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "812bea12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x11cf2ed90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Input -> Retriever -> vectorestoredb\n",
    "\n",
    "vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcf57779",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24b3f9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x11cf2ed90>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template=\"\\nAnswer the question using only the context. If it's not in the context, say you don't know.\\n\\nQuestion:\\n{input}\\n\\nContext:\\n{context}\\n\"), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x11dd3c290>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11dd3de10>, root_client=<openai.OpenAI object at 0x11dd3ced0>, root_async_client=<openai.AsyncOpenAI object at 0x11dd3dbd0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b7c5b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://docs.smith.langchain.com/administration/tutorials/manage_spend your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may\n",
      "want to be more liberal with your usage limits to avoid test failures.\n",
      "Now that our limits are s\n",
      "https://docs.smith.langchain.com/administration/tutorials/manage_spend Understand your current usage‚Äã\n",
      "The first step of any optimization process is to understand current usage. LangSmith gives two ways to do this: Usage Graph\n",
      "and Invoices.\n",
      "Usage Graph‚Äã\n",
      "The usage graph le\n",
      "https://docs.smith.langchain.com/administration/tutorials/manage_spend Optimization 2: limit usage‚Äã\n",
      "In the previous section, we managed data retention settings to optimize existing spend. In this section, we will\n",
      "use usage limits to prevent future overspend.\n",
      "LangSmith ha\n",
      "https://docs.smith.langchain.com/administration/tutorials/manage_spend noteSome of the features mentioned in this guide are not currently available in Enterprise plan due to its\n",
      "custom nature of billing. If you are on Enterprise plan and have questions about cost optimiz\n"
     ]
    }
   ],
   "source": [
    "## Get the response from the LLM\n",
    "response = retrieval_chain.invoke({\"input\": \"LangSmith usage limits?\"})\n",
    "\n",
    "for d in response.get(\"context\", []):\n",
    "    print(d.metadata.get(\"source\"), d.page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14f07587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'LangSmith usage limits?',\n",
       " 'context': [Document(id='ddaa51eb-bd26-4aad-8cd8-7e503cb59bde', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may\\nwant to be more liberal with your usage limits to avoid test failures.\\nNow that our limits are set, we can see that LangSmith shows a maximum spend estimate across all workspaces:'),\n",
       "  Document(id='2eff39bc-1786-4874-85e1-9646a36581ac', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Understand your current usage\\u200b\\nThe first step of any optimization process is to understand current usage. LangSmith gives two ways to do this: Usage Graph\\nand Invoices.\\nUsage Graph\\u200b\\nThe usage graph lets us examine how much of each usage based pricing metric we have consumed lately. It does not directly show\\nspend (which we will see later on our draft invoice).\\nWe can navigate to the Usage Graph under Settings -> Usage and Billing -> Usage Graph.\\n\\nWe see in the graph above that there are two usage metrics that LangSmith charges for:\\n\\nLangSmith Traces (Base Charge)\\nLangSmith Traces (Extended Data Retention Upgrades).'),\n",
       "  Document(id='8c981bc4-8b91-4793-b372-91af21a4957b', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Optimization 2: limit usage\\u200b\\nIn the previous section, we managed data retention settings to optimize existing spend. In this section, we will\\nuse usage limits to prevent future overspend.\\nLangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics we\\'ve\\nbeen tracking on our usage graph. We can use these in tandem to have granular control over spend.\\nTo set limits, we navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the\\nbottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along\\nwith a cost estimate:\\n\\nLets start by setting limits on our production usage, since that is where the majority of spend comes from.\\nSetting a good total traces limit\\u200b\\nPicking the right \"total traces\" limit depends on the expected load of traces that you will send to LangSmith. You should\\nclearly think about your assumptions before setting a limit.\\nFor example:'),\n",
       "  Document(id='a7b894fe-89af-49e9-97f6-7bf573551035', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='noteSome of the features mentioned in this guide are not currently available in Enterprise plan due to its\\ncustom nature of billing. If you are on Enterprise plan and have questions about cost optimization,\\nplease reach out to your sales rep or support@langchain.dev.\\nThis tutorial walks through optimizing your spend on LangSmith. In it, we will learn how to optimize existing spend\\nand prevent future overspend on a realistic real-world example. We will use an existing LangSmith organization with high usage.\\nConcepts can be transferred to your own organization.\\nProblem Setup\\u200b\\nIn this tutorial, we take an existing organization that has three workspaces, one for each deployment stage\\n(Dev, Staging, and Prod):')],\n",
       " 'answer': \"The context does not provide specific numerical values or detailed information regarding the exact usage limits for LangSmith. It only mentions that LangSmith has two usage limits: total traces and extended retention traces, and that these can be set per workspace under the Usage configuration. If you're looking for specific numbers or further details, you might need to check the actual LangSmith platform or contact their support.\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d4b4674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ddaa51eb-bd26-4aad-8cd8-7e503cb59bde', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may\\nwant to be more liberal with your usage limits to avoid test failures.\\nNow that our limits are set, we can see that LangSmith shows a maximum spend estimate across all workspaces:'),\n",
       " Document(id='2eff39bc-1786-4874-85e1-9646a36581ac', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Understand your current usage\\u200b\\nThe first step of any optimization process is to understand current usage. LangSmith gives two ways to do this: Usage Graph\\nand Invoices.\\nUsage Graph\\u200b\\nThe usage graph lets us examine how much of each usage based pricing metric we have consumed lately. It does not directly show\\nspend (which we will see later on our draft invoice).\\nWe can navigate to the Usage Graph under Settings -> Usage and Billing -> Usage Graph.\\n\\nWe see in the graph above that there are two usage metrics that LangSmith charges for:\\n\\nLangSmith Traces (Base Charge)\\nLangSmith Traces (Extended Data Retention Upgrades).'),\n",
       " Document(id='8c981bc4-8b91-4793-b372-91af21a4957b', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Optimization 2: limit usage\\u200b\\nIn the previous section, we managed data retention settings to optimize existing spend. In this section, we will\\nuse usage limits to prevent future overspend.\\nLangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics we\\'ve\\nbeen tracking on our usage graph. We can use these in tandem to have granular control over spend.\\nTo set limits, we navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the\\nbottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along\\nwith a cost estimate:\\n\\nLets start by setting limits on our production usage, since that is where the majority of spend comes from.\\nSetting a good total traces limit\\u200b\\nPicking the right \"total traces\" limit depends on the expected load of traces that you will send to LangSmith. You should\\nclearly think about your assumptions before setting a limit.\\nFor example:'),\n",
       " Document(id='a7b894fe-89af-49e9-97f6-7bf573551035', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='noteSome of the features mentioned in this guide are not currently available in Enterprise plan due to its\\ncustom nature of billing. If you are on Enterprise plan and have questions about cost optimization,\\nplease reach out to your sales rep or support@langchain.dev.\\nThis tutorial walks through optimizing your spend on LangSmith. In it, we will learn how to optimize existing spend\\nand prevent future overspend on a realistic real-world example. We will use an existing LangSmith organization with high usage.\\nConcepts can be transferred to your own organization.\\nProblem Setup\\u200b\\nIn this tutorial, we take an existing organization that has three workspaces, one for each deployment stage\\n(Dev, Staging, and Prod):')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"context\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
