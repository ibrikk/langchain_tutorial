{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5978cfb",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae49ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "# Langsmith Tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a295f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x12024bdd0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x121d75f90> root_client=<openai.OpenAI object at 0x1213339d0> root_async_client=<openai.AsyncOpenAI object at 0x121d75ad0> model_name='gpt-5' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-5\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c7f9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Generative AI is a type of artificial intelligence that creates new content—text, images, audio, video, code, or 3D—by learning patterns from large datasets and then producing original outputs that resemble what it has learned.\\n\\nKey points:\\n- How it works: It models the probability of data (e.g., likely next words or pixels) and samples from that distribution to generate new content. Training is typically self-supervised on massive corpora.\\n- Common model types:\\n  - Large language models (LLMs, usually Transformers) for text and code\\n  - Diffusion models for images, audio, and video\\n  - GANs and VAEs for images and other media\\n- What it can do: Draft and summarize text, write code, create images and designs, synthesize voices and music, generate videos, assist with research and brainstorming, and power conversational assistants.\\n- How it differs from traditional (discriminative) AI: Discriminative models classify or predict labels (P(y|x)); generative models create data by modeling the data distribution (P(x) or P(x|y)).\\n- Benefits: Rapid content creation, ideation, personalization, automation of routine writing/coding/design tasks.\\n- Limitations and risks:\\n  - Hallucinations and factual errors; lacks true understanding\\n  - Bias and offensive outputs reflecting training data\\n  - Copyright, licensing, and data privacy concerns\\n  - Security issues (prompt injection, data leakage)\\n  - Controllability and consistency can be challenging\\n- Improving reliability: human oversight, retrieval-augmented generation (adding trusted sources), fine-tuning, prompt design, constraints and safety filters, and evaluation with domain tests.\\n\\nIn short, generative AI is a pattern-learning engine that can produce convincing new content, powerful for creativity and productivity but requiring care for accuracy, ethics, and safety.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 835, 'prompt_tokens': 12, 'total_tokens': 847, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 448, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C202VOJWM1d4JnxR79ZBQrT6nmBbY', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--61b55aa9-96ea-48be-80d1-05c21396cbd0-0' usage_metadata={'input_tokens': 12, 'output_tokens': 835, 'total_tokens': 847, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 448}}\n"
     ]
    }
   ],
   "source": [
    "## Input and get response from LLM\n",
    "\n",
    "result = llm.invoke(\"What is generative AI?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43dad055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me asnwers based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Provide me asnwers based on the question\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b413bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangSmith is LangChain’s production-grade platform for building, debugging, evaluating, and monitoring LLM applications. It gives you full-stack observability and test/eval tooling so you can ship reliable RAG systems, agents, and chatbots and continuously improve them.\\n\\nWhat it’s for\\n- Debugging: See every call, prompt, tool, retriever, and intermediate step in a hierarchical trace. Inspect inputs/outputs, latencies, costs, and errors.\\n- Evaluation and testing: Create datasets, run automated and human-in-the-loop evaluations, compare model/prompt/logic variants, and prevent regressions in CI.\\n- Monitoring and feedback: Track quality, latency, and cost over time; capture user feedback programmatically; set up dashboards for production.\\n- Collaboration and governance: Org workspaces, projects, RBAC/SSO (enterprise), auditability, PII redaction options, and data export via API.\\n\\nKey capabilities\\n- Tracing and observability\\n  - Hierarchical runs/spans for chains, tools, retrievers, and LLM calls.\\n  - Prompt and response capture, token/latency/cost metrics, errors with stack traces.\\n  - Attach metadata (commit SHA, release, user/session hashes), tags, and artifacts (e.g., retrieved docs).\\n  - Side-by-side run diffs and version comparisons.\\n- Evaluation\\n  - Datasets and examples with ground truth targets and metadata.\\n  - Built-in metrics: exact match, semantic similarity, classification accuracy; LLM-as-judge with custom rubrics; pairwise A/B.\\n  - Batch jobs across datasets; view score distributions and failure cases.\\n- Feedback and monitoring\\n  - Human and programmatic feedback (thumbs, numeric, freeform) attached to runs.\\n  - Quality dashboards by version, segment, or data slice; drill-down from metric to trace.\\n- Integrations\\n  - First-class with LangChain (Python/JS) but framework-agnostic via SDKs and REST API.\\n  - Works with any model/provider; log custom spans for arbitrary code.\\n- Hosting and pricing\\n  - Managed SaaS at langsmith.com; enterprise features include SSO/RBAC and enhanced controls.\\n  - Free and paid tiers exist; check the website for current limits and pricing.\\n\\nQuick start (Python)\\n1) Install and set env\\n- pip install langsmith\\n- export LANGSMITH_API_KEY=YOUR_KEY\\n\\n2) Trace your code\\n- from langsmith import traceable\\n  @traceable(name=\"answer_question\")\\n  def answer(q): \\n      # call your LLM / RAG here\\n      return \"...\"\\n  answer(\"What is LangSmith?\")\\n\\n3) Create a dataset and log ground truth\\n- from langsmith import Client\\n  client = Client()\\n  ds = client.create_dataset(name=\"qa-smoke\")\\n  client.create_example(inputs={\"question\": \"What is LangSmith?\"}, outputs={\"answer\": \"An LLM app observability and eval platform\"}, dataset_id=ds.id)\\n\\n4) Evaluate a new version\\n- Run your function across the dataset and log outputs as runs; trigger evaluators from the UI or via API; compare variants side-by-side.\\n\\nQuick start (JavaScript/TypeScript)\\n- npm i langsmith\\n- export LANGSMITH_API_KEY=YOUR_KEY\\n- import { traceable, Client } from \"langsmith\";\\n  const fn = traceable(async (q) => { /* call model */ return \"...\" });\\n  await fn(\"What is LangSmith?\");\\n  const client = new Client();\\n  const ds = await client.createDataset({ name: \"qa-smoke\" });\\n  await client.createExample({ datasetId: ds.id, inputs: { question: \"...\" }, outputs: { answer: \"...\" } });\\n\\nBest practices\\n- Log context: commit SHA, app/version, dataset slice, user/session (hashed), request IDs.\\n- Capture retrieval context and tool I/O so you can diagnose RAG failures.\\n- Define explicit eval rubrics; combine exact-match/semantic metrics with LLM-judge carefully.\\n- Run offline evals in CI on every change; A/B test in staging; sample real traffic for continuous eval.\\n- Redact PII and secrets before logging; use organization policies/roles appropriately.\\n- Tag runs by experiment and keep prompts/models parameterized for easy variant testing.\\n\\nWhen to use it\\n- You have anything beyond a trivial single-call prompt.\\n- You need to compare prompts/models, stabilize RAG quality, or monitor cost/latency/quality in production.\\n- You want a repeatable eval workflow integrated with your code and CI/CD.\\n\\nAlternatives and complements\\n- Other observability/eval tools: HoneyHive, Humanloop, Arize Phoenix, Weights & Biases (Prompts), Helicone, Braintrust, OpenAI Evals/Trace. Choice depends on stack, hosting, and workflow needs.\\n\\nIf you share your stack (Python/JS), framework (LangChain or not), and use case (chat, RAG, agents), I can sketch an integration plan and example evaluators tailored to it.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 3294, 'prompt_tokens': 34, 'total_tokens': 3328, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 2240, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C202elMbxekKZK3l3y1VtqKMPfpvR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--5b7a52ed-3401-4733-9f6d-c29c1921c8e5-0' usage_metadata={'input_tokens': 34, 'output_tokens': 3294, 'total_tokens': 3328, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 2240}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"Can you tell me about Langsmith?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ee105b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57380b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is LangChain’s platform for building, evaluating, and monitoring LLM applications. It gives you end-to-end tooling to trace what your app is doing, test changes safely, measure quality and cost, and keep production behavior under control.\n",
      "\n",
      "What it does\n",
      "- Tracing and observability: Automatically captures step-by-step traces of prompts, model/tool calls, intermediate states, latencies, token and cost usage. Great for debugging complex chains, agents, and RAG pipelines.\n",
      "- Evaluation and testing: Create datasets (from production traces or uploads), define ground truth and/or model-graded evaluators, run experiments across models/prompts, and set pass/fail thresholds for CI. Supports both LLM-as-judge and custom Python/JS evaluators.\n",
      "- Regression and CI: Compare new prompts/models/parameters against a baseline. Gate deployments with test suites in GitHub Actions or other CI.\n",
      "- Dataset and feedback management: Curate datasets, label expected outputs, attach human feedback, and slice results by metadata to find failure modes.\n",
      "- Prompt and experiment workflows: Iterate on prompts and parameters, A/B compare runs, and promote the best versions.\n",
      "- Production monitoring: Track latency, errors, costs, and quality signals over time. Drill into outliers with full traces. Set up alerts/webhooks.\n",
      "- Collaboration and governance: Organize work in projects, share runs, control data retention, and use enterprise features (SSO, RBAC, SOC 2) on paid plans.\n",
      "\n",
      "Who uses it\n",
      "- Teams building agents, RAG, and multi-step workflows who need visibility and quality gates.\n",
      "- Orgs that want reproducible evaluations and safe deployment pipelines for LLM changes.\n",
      "\n",
      "How it integrates\n",
      "- First-class with LangChain and LangGraph: just set env vars to get tracing.\n",
      "- Python and JavaScript SDKs for use with or without LangChain.\n",
      "- Works with popular model providers (OpenAI, Anthropic, Google/Vertex, Azure OpenAI, etc.).\n",
      "- Can ingest production traces and turn them into datasets for offline evaluation.\n",
      "\n",
      "Typical workflow\n",
      "1) Instrument: Enable tracing in dev to capture runs and debug bottlenecks.\n",
      "2) Curate data: Collect real queries and create labeled datasets.\n",
      "3) Evaluate: Add model-graded and/or rule-based evaluators; run experiments across prompts/models.\n",
      "4) Automate: Put tests in CI to prevent quality regressions and cost blowups.\n",
      "5) Monitor: Watch production metrics and drill into traces when issues occur.\n",
      "\n",
      "Getting started (Python example)\n",
      "- Install: pip install langsmith langchain-openai\n",
      "- Set env vars:\n",
      "  - LANGCHAIN_TRACING_V2=true\n",
      "  - LANGCHAIN_API_KEY=your_key\n",
      "  - (optional) LANGCHAIN_PROJECT=my-app\n",
      "- Minimal code:\n",
      "  from langchain_openai import ChatOpenAI\n",
      "  from langchain.prompts import ChatPromptTemplate\n",
      "\n",
      "  llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
      "  prompt = ChatPromptTemplate.from_template(\"Translate to French: {text}\")\n",
      "  chain = prompt | llm\n",
      "\n",
      "  result = chain.invoke({\"text\": \"Hello world\"})\n",
      "  print(result.content)  # Trace appears in LangSmith\n",
      "\n",
      "JS is similar: npm i langsmith @langchain/openai, set the same env vars, and run your chain.\n",
      "\n",
      "Evaluation sketch\n",
      "- Turn traces into a dataset (select runs in the UI or upload JSONL).\n",
      "- Add evaluators (e.g., correctness, relevance, harmfulness) — model-graded or custom.\n",
      "- Run experiments comparing models/prompts; view scores, costs, and error slices.\n",
      "- Gate merges via CLI/CI so changes must meet your thresholds.\n",
      "\n",
      "Security and pricing\n",
      "- Data controls include project scoping and retention settings; enterprise plans add SSO, RBAC, and compliance (e.g., SOC 2). Avoid sending secrets in prompts/metadata.\n",
      "- Free tier available; paid plans scale with usage, seats, and retention. Enterprise options exist.\n",
      "\n",
      "Alternatives/adjacent tools\n",
      "- Similar goals to platforms like HoneyHive, Humanloop, Arize Phoenix, PromptLayer, or OpenAI Evals; LangSmith is tightly integrated with LangChain/LangGraph and emphasizes deep tracing plus eval/CI.\n",
      "\n",
      "If you share your stack (Python/JS, providers, whether you use LangChain/LangGraph), I can give a tailored setup snippet and an example evaluator for your use case.\n"
     ]
    }
   ],
   "source": [
    "## stroutput Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\": \"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
